{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectra Encoder: Transformer\n",
    "\n",
    "Primary reference: https://www.nature.com/articles/s42004-023-00932-3#Sec19\n",
    "\n",
    "Using this paper as a framework, the purpose of this transformer is to take input GC-MS spectral data and output embeddings to be passed to the SMILES decoder. The reference used images of GC-MS data and implemented a CNN; we intend to use a transformer instead.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supplemental references:\n",
    "https://jalammar.github.io/illustrated-transformer/ (Illustrated overview of Transformer function)\n",
    "\n",
    "https://nlp.seas.harvard.edu/2018/04/03/attention.html (Harvard coding annotation of original Transformation paper)\n",
    "\n",
    "https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch (Datacamp Transformer tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook overview:\n",
    "1. Define model building blocks\n",
    "2. Encoding\n",
    "3. Decoding\n",
    "4. Training\n",
    "5. Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# finding unique characters in the SMILES column of training data \n",
    "\n",
    "unique_characters = set() \n",
    "\n",
    "with open('dataset/filtered_gc_spec.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)  \n",
    "    for row in reader:\n",
    "        for char in row[\"SMILES\"]:\n",
    "            unique_characters.add(char)  # Add each character to the set\n",
    "\n",
    "print(len(unique_characters))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517627\n"
     ]
    }
   ],
   "source": [
    "# finding unique tuples in the spectral training data\n",
    "unique_tuples = set()  \n",
    "\n",
    "with open('dataset/filtered_gc_spec.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        spectrum_data = row[\"Spectrum\"]\n",
    "        tuples = spectrum_data.split() \n",
    "        for tup in tuples:\n",
    "            if ':' in tup and tup.count(':') == 1: \n",
    "                unique_tuples.add(tup)\n",
    "\n",
    "print(len(unique_tuples))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to go from a \"vocabulary\" of 517627 unique tuples to 45 unique characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the SMILES data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2186378/396997768.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "# Requriments - transformers, tokenizers\n",
    "# Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset.\n",
    "# The vocab may be expanded in the near future\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import pkg_resources\n",
    "from typing import List\n",
    "from transformers import BertTokenizer\n",
    "from logging import getLogger\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\"\"\"\n",
    "SMI_REGEX_PATTERN: str\n",
    "    SMILES regex pattern for tokenization. Designed by Schwaller et. al.\n",
    "\n",
    "References\n",
    "\n",
    ".. [1]  Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A. Hunter, Costas Bekas, and Alpha A. Lee\n",
    "        ACS Central Science 2019 5 (9): Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction\n",
    "        1572-1583 DOI: 10.1021/acscentsci.9b00576\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "SMI_REGEX_PATTERN = r\"\"\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|\n",
    "#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n",
    "\n",
    "# add vocab_file dict\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
    "\n",
    "\n",
    "class SmilesTokenizer(BertTokenizer):\n",
    "    def __init__(self, vocab_file: str = '', max_len: int = 512, **kwargs):\n",
    "        \"\"\"Constructs a SmilesTokenizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_file: str\n",
    "            Path to a SMILES character per line vocabulary file.\n",
    "            Default vocab file is found in deepchem/feat/tests/data/vocab.txt\n",
    "        max_len: int\n",
    "            Maximum length for tokenized sequences.\n",
    "        \"\"\"\n",
    "        self.max_len = max_len\n",
    "\n",
    "        super().__init__(vocab_file=vocab_file, max_len=max_len, **kwargs)\n",
    "        \n",
    "        # take into account special tokens in max length\n",
    "        #self.max_len_single_sentence = self.max_len - 2\n",
    "        #self.max_len_sentences_pair = self.max_len - 3\n",
    "\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a vocab file at path '{}'.\".format(vocab_file)\n",
    "            )\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.highest_unused_index = max(\n",
    "            [i for i, v in enumerate(self.vocab.keys()) if v.startswith(\"[unused\")])\n",
    "        self.ids_to_tokens = collections.OrderedDict(\n",
    "            [(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.basic_tokenizer = BasicSmilesTokenizer()\n",
    "        self.init_kwargs[\"max_len\"] = self.max_len\n",
    "\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "      return len(self.vocab)\n",
    "\n",
    "    @property\n",
    "    def vocab_list(self):\n",
    "      return list(self.vocab.keys())\n",
    "\n",
    "    def _tokenize(self, text: str):\n",
    "      \"\"\"\n",
    "          Tokenize a string into a list of tokens.\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          text: str\n",
    "              Input string sequence to be tokenized.\n",
    "          \"\"\"\n",
    "\n",
    "      split_tokens = [token for token in self.basic_tokenizer.tokenize(text)]\n",
    "      return split_tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "      \"\"\"\n",
    "          Converts a token (str/unicode) in an id using the vocab.\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          token: str\n",
    "              String token from a larger sequence to be converted to a numerical id.\n",
    "          \"\"\"\n",
    "\n",
    "      return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "      \"\"\"\n",
    "          Converts an index (integer) in a token (string/unicode) using the vocab.\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          index: int\n",
    "              Integer index to be converted back to a string-based token as part of a larger sequence.\n",
    "          \"\"\"\n",
    "\n",
    "      return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens: List[str]):\n",
    "      \"\"\" Converts a sequence of tokens (string) in a single string.\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          tokens: List[str]\n",
    "              List of tokens for a given string sequence.\n",
    "\n",
    "          Returns\n",
    "          -------\n",
    "          out_string: str\n",
    "              Single string from combined tokens.\n",
    "          \"\"\"\n",
    "\n",
    "      out_string: str = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
    "      return out_string\n",
    "\n",
    "    def add_special_tokens_ids_single_sequence(self, token_ids: List[int]):\n",
    "      \"\"\"\n",
    "          Adds special tokens to the a sequence for sequence classification tasks.\n",
    "          A BERT sequence has the following format: [CLS] X [SEP]\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "\n",
    "          token_ids: list[int]\n",
    "              list of tokenized input ids. Can be obtained using the encode or encode_plus methods.\n",
    "          \"\"\"\n",
    "\n",
    "      return [self.cls_token_id] + token_ids + [self.sep_token_id]\n",
    "\n",
    "    def add_special_tokens_single_sequence(self, tokens: List[str]):\n",
    "      \"\"\"\n",
    "          Adds special tokens to the a sequence for sequence classification tasks.\n",
    "          A BERT sequence has the following format: [CLS] X [SEP]\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          tokens: List[str]\n",
    "              List of tokens for a given string sequence.\n",
    "\n",
    "          \"\"\"\n",
    "      return [self.cls_token] + tokens + [self.sep_token]\n",
    "\n",
    "    def add_special_tokens_ids_sequence_pair(self, token_ids_0: List[int],\n",
    "                                            token_ids_1: List[int]) -> List[int]:\n",
    "      \"\"\"\n",
    "          Adds special tokens to a sequence pair for sequence classification tasks.\n",
    "          A BERT sequence pair has the following format: [CLS] A [SEP] B [SEP]\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          token_ids_0: List[int]\n",
    "              List of ids for the first string sequence in the sequence pair (A).\n",
    "\n",
    "          token_ids_1: List[int]\n",
    "              List of tokens for the second string sequence in the sequence pair (B).\n",
    "          \"\"\"\n",
    "\n",
    "      sep = [self.sep_token_id]\n",
    "      cls = [self.cls_token_id]\n",
    "\n",
    "      return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def add_padding_tokens(self,\n",
    "                          token_ids: List[int],\n",
    "                          length: int,\n",
    "                          right: bool = True) -> List[int]:\n",
    "      \"\"\"\n",
    "          Adds padding tokens to return a sequence of length max_length.\n",
    "          By default padding tokens are added to the right of the sequence.\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          token_ids: list[int]\n",
    "              list of tokenized input ids. Can be obtained using the encode or encode_plus methods.\n",
    "\n",
    "          length: int\n",
    "\n",
    "          right: bool (True by default)\n",
    "\n",
    "          Returns\n",
    "          ----------\n",
    "          token_ids :\n",
    "              list of tokenized input ids. Can be obtained using the encode or encode_plus methods.\n",
    "\n",
    "          padding: int\n",
    "              Integer to be added as padding token\n",
    "\n",
    "          \"\"\"\n",
    "      padding = [self.pad_token_id] * (length - len(token_ids))\n",
    "\n",
    "      if right:\n",
    "        return token_ids + padding\n",
    "      else:\n",
    "        return padding + token_ids\n",
    "\n",
    "    def save_vocabulary(\n",
    "        self, vocab_path: str\n",
    "    ):  # -> tuple[str]: doctest issue raised with this return type annotation\n",
    "      \"\"\"\n",
    "          Save the tokenizer vocabulary to a file.\n",
    "\n",
    "          Parameters\n",
    "          ----------\n",
    "          vocab_path: obj: str\n",
    "              The directory in which to save the SMILES character per line vocabulary file.\n",
    "              Default vocab file is found in deepchem/feat/tests/data/vocab.txt\n",
    "\n",
    "          Returns\n",
    "          ----------\n",
    "          vocab_file: :obj:`Tuple(str)`:\n",
    "              Paths to the files saved.\n",
    "              typle with string to a SMILES character per line vocabulary file.\n",
    "              Default vocab file is found in deepchem/feat/tests/data/vocab.txt\n",
    "\n",
    "          \"\"\"\n",
    "      index = 0\n",
    "      if os.path.isdir(vocab_path):\n",
    "        vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "      else:\n",
    "        vocab_file = vocab_path\n",
    "      with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "        for token, token_index in sorted(\n",
    "            self.vocab.items(), key=lambda kv: kv[1]):\n",
    "          if index != token_index:\n",
    "            logger.warning(\n",
    "                \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                \" Please check that the vocabulary is not corrupted!\".format(\n",
    "                    vocab_file))\n",
    "            index = token_index\n",
    "          writer.write(token + \"\\n\")\n",
    "          index += 1\n",
    "      return (vocab_file,)\n",
    "\n",
    "\n",
    "class BasicSmilesTokenizer(object):\n",
    "  \"\"\"\n",
    "\n",
    "    Run basic SMILES tokenization using a regex pattern developed by Schwaller et. al. This tokenizer is to be used\n",
    "    when a tokenizer that does not require the transformers library by HuggingFace is required.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\n",
    "    >>> tokenizer = BasicSmilesTokenizer()\n",
    "    >>> print(tokenizer.tokenize(\"CC(=O)OC1=CC=CC=C1C(=O)O\"))\n",
    "    ['C', 'C', '(', '=', 'O', ')', 'O', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', 'C', '(', '=', 'O', ')', 'O']\n",
    "\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1]  Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A. Hunter, Costas Bekas, and Alpha A. Lee\n",
    "            ACS Central Science 2019 5 (9): Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction\n",
    "            1572-1583 DOI: 10.1021/acscentsci.9b00576\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "  def __init__(self, regex_pattern: str = SMI_REGEX_PATTERN):\n",
    "    \"\"\" Constructs a BasicSMILESTokenizer.\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        regex: string\n",
    "            SMILES token regex\n",
    "\n",
    "        \"\"\"\n",
    "    self.regex_pattern = regex_pattern\n",
    "    self.regex = re.compile(self.regex_pattern)\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    \"\"\" Basic Tokenization of a SMILES.\n",
    "        \"\"\"\n",
    "    tokens = [token for token in self.regex.findall(text)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "  vocab = collections.OrderedDict()\n",
    "  with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "    tokens = reader.readlines()\n",
    "  for index, token in enumerate(tokens):\n",
    "    token = token.rstrip(\"\\n\")\n",
    "    vocab[token] = index\n",
    "  return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = 'vocab.txt'\n",
    "tokenizer = SmilesTokenizer(vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GC-MS Spectra for input: 100\n",
      "Number of SMILES sequences for output: 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('dataset/filtered_gc_spec.csv')\n",
    "input_MS = pd.Series(data[\"Spectrum\"][:100])\n",
    "output_SMILES = pd.Series(data[\"SMILES\"][:100])\n",
    "\n",
    "assert len(input_MS) == len(output_SMILES)  # sanity check to ensure correct loading\n",
    "\n",
    "# filter input by length of SMILES (<77 as per SMILES encoder)\n",
    "output_SMILES_filtered = output_SMILES[output_SMILES.str.len() < 77]\n",
    "input_MS_filtered = input_MS.loc[output_SMILES_filtered.index]\n",
    "\n",
    "assert len(input_MS_filtered) == len(output_SMILES_filtered)  # sanity check to ensure correct filtering\n",
    "\n",
    "print(f\"Number of GC-MS Spectra for input: {len(input_MS_filtered)}\")\n",
    "print(f\"Number of SMILES sequences for output: {len(output_SMILES_filtered)}\")\n",
    "\n",
    "smiles_list = output_SMILES_filtered.tolist()\n",
    "\n",
    "# tokenize SMILES data\n",
    "tokenized_smiles = [tokenizer.encode(smiles) for smiles in smiles_list]\n",
    "\n",
    "# set max length of SMILES to 64\n",
    "max_length = 64\n",
    "\n",
    "def pad_seq(tokens, max_length):\n",
    "    return tokens + [0] * (max_length - len(tokens))  # adding padding (zeroes)\n",
    "\n",
    "# pad all SMILES entries and convert to tensor\n",
    "padded_smiles = [pad_seq(tokens, max_length) for tokens in tokenized_smiles]\n",
    "smiles_tensor = torch.tensor(padded_smiles, dtype=torch.long)\n",
    "\n",
    "# converting MS data to tensor\n",
    "def spec_2_tensor(spectrum, max_length):\n",
    "    spectrum_tuples = [(float(mz), float(intensity)) for mz, intensity in (item.split(\":\") for item in spectrum.split())]\n",
    "    return spectrum_tuples[:max_length] + [(0, 0)] * (max_length - len(spectrum_tuples))  # padding as zeroes\n",
    "\n",
    "input_MS_data = input_MS_filtered.apply(lambda x: spec_2_tensor(x, max_length))\n",
    "ms_tensor = torch.tensor(input_MS_data.tolist(), dtype=torch.float32)\n",
    "\n",
    "# linear layer to map from 2 features to vocab size\n",
    "src_vocab_size = 64\n",
    "linear_layer = nn.Linear(2, src_vocab_size)\n",
    "\n",
    "# flatten, transform, and reshape back\n",
    "ms_tensor_flat = ms_tensor.view(-1, 2)  # flatten for batch processing\n",
    "ms_tensor_transformed = linear_layer(ms_tensor_flat)\n",
    "ms_tensor_indices = ms_tensor_transformed.argmax(dim=1)  # select the index with the highest value\n",
    "ms_tensor_indices = ms_tensor_indices.view(len(input_MS_filtered), max_length)  # reshape back\n",
    "\n",
    "# train/test split\n",
    "ms_train, ms_test, smiles_train, smiles_test = train_test_split(ms_tensor_indices.numpy(), smiles_tensor.numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# convert back to tensors\n",
    "ms_train = torch.tensor(ms_train, dtype=torch.long)\n",
    "ms_test = torch.tensor(ms_test, dtype=torch.long)\n",
    "smiles_train = torch.tensor(smiles_train, dtype=torch.long)\n",
    "smiles_test = torch.tensor(smiles_test, dtype=torch.long)\n",
    "\n",
    "# create datasets and dataloaders\n",
    "batch_size_train = 5\n",
    "batch_size_test = 5\n",
    "\n",
    "train_dataset = TensorDataset(ms_train, smiles_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(ms_test, smiles_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating model\n",
      "------------------------- Epoch 1 -------------------------\n",
      "Training loss: 3.5624\n",
      "Validation loss: 3.0133\n",
      "\n",
      "------------------------- Epoch 2 -------------------------\n",
      "Training loss: 2.9007\n",
      "Validation loss: 2.6253\n",
      "\n",
      "------------------------- Epoch 3 -------------------------\n",
      "Training loss: 2.5886\n",
      "Validation loss: 2.3684\n",
      "\n",
      "------------------------- Epoch 4 -------------------------\n",
      "Training loss: 2.3817\n",
      "Validation loss: 2.1973\n",
      "\n",
      "------------------------- Epoch 5 -------------------------\n",
      "Training loss: 2.2283\n",
      "Validation loss: 2.0614\n",
      "\n",
      "------------------------- Epoch 6 -------------------------\n",
      "Training loss: 2.1030\n",
      "Validation loss: 1.9429\n",
      "\n",
      "------------------------- Epoch 7 -------------------------\n",
      "Training loss: 1.9904\n",
      "Validation loss: 1.8249\n",
      "\n",
      "------------------------- Epoch 8 -------------------------\n",
      "Training loss: 1.8837\n",
      "Validation loss: 1.7295\n",
      "\n",
      "------------------------- Epoch 9 -------------------------\n",
      "Training loss: 1.7870\n",
      "Validation loss: 1.6298\n",
      "\n",
      "------------------------- Epoch 10 -------------------------\n",
      "Training loss: 1.7051\n",
      "Validation loss: 1.5506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "#positional encoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model)\n",
    "        \n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_tokens, dim_model, num_heads, num_encoder_layers, num_decoder_layers, dropout_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(dim_model=dim_model, dropout_p=dropout_p, max_len=5000)\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        \n",
    "        # Setting batch_first=True for better performance with nested tensors\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model, \n",
    "            nhead=num_heads, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            num_decoder_layers=num_decoder_layers, \n",
    "            dropout=dropout_p, \n",
    "            batch_first=True  # setting this to True for nested tensors, error otherwise\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "\n",
    "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        out = self.out(transformer_out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        mask = torch.tril(torch.ones(size, size) == 1)\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        return (matrix == pad_token)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(num_tokens=src_vocab_size, dim_model=8, num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1).to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        y_input = y[:, :-1]\n",
    "        y_expected = y[:, 1:]\n",
    "        \n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "        pred = model(X, y_input, tgt_mask)\n",
    "        \n",
    "        # Flatten the predictions and expected outputs for computing the loss\n",
    "        pred = pred.view(-1, pred.size(-1))\n",
    "        y_expected = y_expected.contiguous().view(-1)\n",
    "        \n",
    "        loss = loss_fn(pred, y_expected)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_input = y[:, :-1]\n",
    "            y_expected = y[:, 1:]\n",
    "            \n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "            pred = model(X, y_input, tgt_mask)\n",
    "            \n",
    "            # Flatten the predictions and expected outputs for computing the loss\n",
    "            pred = pred.view(-1, pred.size(-1))\n",
    "            y_expected = y_expected.contiguous().view(-1)\n",
    "            \n",
    "            loss = loss_fn(pred, y_expected)\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list.append(validation_loss)\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n",
    "\n",
    "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_loader, test_loader, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msse-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
